{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["CE080_Vedant_Parikh\n","\n","Machine Learning\n","\n","Lab : 06"],"metadata":{"id":"UYs2iiJoxpRc"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcSh8uEjlLnT","executionInfo":{"status":"ok","timestamp":1677113867642,"user_tz":-330,"elapsed":13468,"user":{"displayName":"CE080_Vedant_Parikh","userId":"04185109320538044022"}},"outputId":"897e4e04-4383-43d7-f365-a46b9e1f4983"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["14996     twaiting wife come stairs knock tivo. leaves t...\n","14329                                    can't see #asot400\n","816323    @serenajwilliams find k. rowland http://twitte...\n","802741    countdown: 7:16 h vacation start 8 days off, b...\n","1032                                  waiting kelly's call.\n","                                ...                        \n","816868                                 sonny chance youtube\n","813750    @kartar it's not like lot do, many people rush...\n","11228                        still pain tonsillectomy worst\n","806108    @ohsnapandrew goodmorning lovely, how's texas?...\n","819497    @linzking fantastic news, hoping successful ti...\n","Name: text, Length: 10000, dtype: object"]},"metadata":{},"execution_count":2}],"source":["import pandas as pd\n","from nltk.corpus import stopwords\n","import string\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import classification_report\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user_id\", \"text\"]\n","df = pd.read_csv(\"/content/drive/MyDrive/DDU/Semester-6/Labs_ML/training.1600000.processed.noemoticon.csv\", encoding = \"latin\", names = columns)\n","df[\"sentiment\"] = df[\"sentiment\"].replace(4, 1)\n","df['sentiment'].unique()\n","data_pos = df[df['sentiment'] == 1]\n","data_neg = df[df['sentiment'] == 0]\n","data_pos = data_pos.iloc[:int(20000)]\n","data_neg = data_neg.iloc[:int(20000)]\n","dataset = pd.concat([data_pos, data_neg]) \n","dataset['text'] = dataset['text'].str.lower()\n","dataset['text'].tail()\n","stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n","             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n","             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n","             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n","             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n","             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n","             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n","             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n","             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n","             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n","             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n","             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n","             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n","             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n","             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n","STOPWORDS = set(stopwordlist)\n","def cleaning_stopwords(text):\n","    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n","dataset['text'] = dataset['text'].apply(lambda text: cleaning_stopwords(text))\n","dataset['text'].sample(10000)"]}]}